# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HrxfRSotnazi_LcdzQLXDXvv3kM7xCmI
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import re
import nltk 
nltk.download('stopwords')

import matplotlib.pyplot as plt
# %matplotlib inline

#Load the data
from google.colab import files
files.upload()

df1=pd.read_csv('Tweets.csv')

df1.shape

df1.head()

plot_size=plt.rcParams["figure.figsize"]
print(plot_size[0])
print(plot_size[1])

plot_size[0]=8
plot_size[1]=6
plt.rcParams["figure.figsize"]=plot_size

#Exploration of data
df1.airline.value_counts().plot(kind='pie', autopct="%1.0f%%")

df1.airline_sentiment.value_counts().plot(kind='pie',autopct="1.0f%%",colors=['red','yellow','green'])

airline_sentiment=df1.groupby(['airline','airline_sentiment']).airline_sentiment.count().unstack()
 airline_sentiment.plot(kind='bar')

import seaborn as sns
sns.barplot(x='airline_sentiment',y='airline_sentiment_confidence',data=df1)

#DATA CLEANING

features=df1.iloc[:,10].values
labels=df1.iloc[:,1].values

processed_features=[]
 for sentence in range(0,len(features)):
   #Remove all the special characters
   processed_feature=re.sub(r'\W',' ',str(features[sentence]))

   #Remove all single characters
   processed_feature=re.sub(r's\+[a-zA-Z]\s',' ',processed_feature)

   #Remmove single characters from the start
   processed_feature=re.sub(r'\^[a-zA-Z]\s+',' ',processed_feature)

   #substituting multiple spaces with single space
   processed_feature=re.sub(r'\s+',' ',processed_feature,flags=re.I)

   #Removing prefixed 'b'
   processed_feature=re.sub(r'^b\s+',' ',processed_feature)

   #Converting to lowercase
   processed_feature=processed_feature.lower()

   processed_features.append(processed_feature)

#TF=Term frequnecy
#IDF=Inverse Document Frequency
#Calculated using Scikit-Learn Library
from nltk.corpus import stopwords
stopwords.words('english')
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer =  TfidfVectorizer(max_features=2500,min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features=vectorizer.fit_transform(processed_features).toarray()

#Dividing data into training and testing sets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(processed_features,labels,test_size=0.2,random_state=0)

from sklearn.ensemble import RandomForestClassifier

text_classifier = RandomForestClassifier(n_estimators=200,random_state=0)
text_classifier.fit(X_train,y_train)

predictions = text_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score

print(confusion_matrix(y_test,predictions))
print('accuracy score', accuracy_score(y_test,predictions))